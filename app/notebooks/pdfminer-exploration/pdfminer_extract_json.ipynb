{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This can be skipped\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"last_expr_or_assign\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update this path to the folder where the PDFs are stored\n",
    "pdf_folder=\"../bem_pdfs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get some basic info about the pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "\n",
    "my_pdf=f\"{pdf_folder}/100.pdf\"\n",
    "fp = open(my_pdf, \"rb\")\n",
    "doc = PDFDocument(PDFParser(fp))\n",
    "\n",
    "doc.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdf_tagextractor\n",
    "pdf_info = pdf_tagextractor.get_pdf_info(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the very useful outline (i.e., heading hierarchy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outline = pdf_tagextractor.extract_outline(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out-of-the-box results using their TagExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfdevice import TagExtractor\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from io import BytesIO\n",
    "import xml.dom.minidom as minidom\n",
    "\n",
    "# Extracted from pdfminer.high_level.py:extract_text_to_fp()\n",
    "def interpreter_for_builtin_tag_extractor(output_io, output_codec: str = \"utf-8\"):\n",
    "    rsrcmgr = PDFResourceManager(caching=True)\n",
    "    pdf_device = TagExtractor(rsrcmgr, outfp=output_io, codec=output_codec)\n",
    "    return PDFPageInterpreter(rsrcmgr, pdf_device)\n",
    "\n",
    "def extract_xml(doc: PDFDocument, validate_xml: bool = False):\n",
    "    output_io = BytesIO()\n",
    "    interpreter = interpreter_for_builtin_tag_extractor(output_io)\n",
    "    for (pageno, page) in enumerate(PDFPage.create_pages(doc), start=1):\n",
    "        # print(\"page\", pageno, page.pageid)\n",
    "        # As the interpreter reads the PDF, it will call methods on interpreter.device,\n",
    "        # which will write to output_io\n",
    "        interpreter.process_page(page)\n",
    "\n",
    "    # After done writing to output_io, go back to the beginning so we can read() it\n",
    "    output_io.seek(0)\n",
    "    # Wrap all tags in a root tag\n",
    "    xml_string = \"<pdf>\" + output_io.read().decode() + \"</pdf>\"\n",
    "\n",
    "    # Paste this string into https://jsonformatter.org/xml-formatter\n",
    "    # and click \"Format\" to diagnose any XML validation issues\n",
    "    # print(xml_string)\n",
    "\n",
    "    if validate_xml:\n",
    "        minidom.parseString(xml_string) # nosec\n",
    "\n",
    "    return xml_string\n",
    "\n",
    "orig_xml_string = extract_xml(doc, validate_xml=True)\n",
    "len(orig_xml_string.splitlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.dom.minidom\n",
    "dom = xml.dom.minidom.parseString(orig_xml_string) # nosec\n",
    "print(dom.toprettyxml(indent=\"  \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following PDFs have errors using original TagExtractor:\n",
    "```\n",
    "Error in extracted xml for 101.pdf: mismatched tag: line 2, column 9032\n",
    "Error in extracted xml for 105.pdf: mismatched tag: line 5, column 2360\n",
    "Error in extracted xml for 203.pdf: mismatched tag: line 4, column 3184\n",
    "Error in extracted xml for 210.pdf: 13\n",
    "Error in extracted xml for 225.pdf: mismatched tag: line 34, column 4550\n",
    "Error in extracted xml for 230A.pdf: 7\n",
    "Error in extracted xml for 400.pdf: mismatched tag: line 72, column 5552\n",
    "Error in extracted xml for 554.pdf: 28\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use custom BemTagExtractor and postprocess XML into JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bem_parser = pdf_tagextractor.BemPdfParser(my_pdf)\n",
    "xml_string = bem_parser.extract_xml(validate_xml=not True)\n",
    "len(orig_xml_string.splitlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.dom.minidom\n",
    "dom = xml.dom.minidom.parseString(xml_string) # nosec\n",
    "print(dom.toprettyxml(indent=\"  \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "ann_texts = bem_parser.to_annotated_texts(xml_string)\n",
    "pprint.pprint(ann_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bem_parser.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test all PDFs and save JSON to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capturing notes about problematic PDFs\n",
    "\n",
    "pdfs_with_issues = [\n",
    "    # BEM-specific PDFs with issues\n",
    "    # Manually fixed by removing duplicate consecutive headings\n",
    "    \"106.pdf\", # outline has multiple \"MSA waiver service agents\" headings in outline (due to table overflow?) but not in the TEXT\n",
    "\n",
    "    ## PDFs with issues using the original TagExtractor\n",
    "\n",
    "    # The following have missing closing tags\n",
    "    \"101.pdf\", # missing closing P tag in table on page 2\n",
    "#\t\t<P MCID=\"55\">166</P>\n",
    "#\t\t<P MCID=\"56\">\n",
    "#\t\t\t<P MCID=\"57\"></P>\n",
    "#\t\t\t<P MCID=\"58\"></P>\n",
    "\n",
    "    ## Fixed by not allowing nested Span tags\n",
    "    \"105.pdf\", # missing closing SPAN tag (nested SPAN in numbered list on page 5)\n",
    "#\t\t<Span Lang=\"en-US\" MCID=\"2\">1. BEM 150 addresses MA for SSI recipients and persons appealing an SSI disability termination. The other SSI-related categories must be considered in the following order: BEM 154, Special Disabled Children \n",
    "#\t\t\t<Span Lang=\"en-US\" MCID=\"3\">2. Special categories: </Span>\n",
    "\n",
    "    \"203.pdf\", # missing closing SPAN tag (nested SPAN in list on page 4)\n",
    "# \t\t<Span Lang=\"en-US\" MCID=\"18\">â€¢\n",
    "#\t\t\t<Span Lang=\"en-US\" MCID=\"19\"> Above individual&#x27;s ID, date of birth, race, sex and SSN. </Span>\n",
    "\n",
    "    \"225.pdf\", # missing closing SPAN tag (due to nested SPAN in table on page 34?)\n",
    "# \t\t<Span Lang=\"en-US\" MCID=\"43\">Yes \n",
    "#\t\t\t<Span Lang=\"en-US\" MCID=\"44\">Yes </Span>\n",
    "\n",
    "    \"400.pdf\", # missing closing SPAN tag (due to nested SPAN in table on page 72? plus table within table!)\n",
    "# \t\t<P MCID=\"6\"></P>\n",
    "#\t\t<Span Lang=\"en-US\" MCID=\"7\">Client has: \n",
    "#\t\t\t<P MCID=\"8\"></P>\n",
    "#\t\t\t<Span Lang=\"en-US\" MCID=\"9\">$2,500 Savings Account </Span>\n",
    "    ]\n",
    "pdfs_with_extra_end_tag = [\n",
    "    # Fixed by ignoring call to end_tag()\n",
    "    \"210.pdf\", # page 14 of 20: assert self._stack, str(self.pageno)\n",
    "    \"230A.pdf\", # page 8: assert self._stack, str(self.pageno)\n",
    "    \"554.pdf\", # page 29: assert self._stack, str(self.pageno)\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_ALL_PDFS = False\n",
    "\n",
    "import os\n",
    "if TEST_ALL_PDFS:\n",
    "    import jsonpickle\n",
    "\n",
    "for file in sorted(os.listdir(pdf_folder)):\n",
    "    if not TEST_ALL_PDFS:\n",
    "        break\n",
    "    if file.endswith(\".pdf\"):\n",
    "        pdf_filename = os.path.join(pdf_folder, file)\n",
    "        print(file)\n",
    "        fp = open(pdf_filename, \"rb\")\n",
    "        try:\n",
    "            test_original_tagextractor = False\n",
    "            if test_original_tagextractor:\n",
    "                if file in pdfs_with_issues:\n",
    "                    continue\n",
    "                doc = PDFDocument(PDFParser(fp))\n",
    "                orig_xml_string = extract_xml(doc, validate_xml=True)\n",
    "            else:\n",
    "                if not os.path.exists(f\"{file}.json\"):\n",
    "                    bem_parser = pdf_tagextractor.BemPdfParser(file)\n",
    "                    if file == \"106.pdf\":\n",
    "                        for i in range(3):\n",
    "                            print(\"Removed duplicate heading: \", bem_parser.parsing_context.heading_stack.pop(0))                    \n",
    "                    xml_string = bem_parser.extract_xml(validate_xml=True)\n",
    "                    ann_texts = bem_parser.to_annotated_texts(xml_string)\n",
    "                    with open(f\"{file}.json\", 'w') as fp:\n",
    "                        fp.write(jsonpickle.encode(ann_texts, indent=2, make_refs=False, unpicklable=False))\n",
    "                bem_parser.close()\n",
    "                # break\n",
    "        except Exception as e:\n",
    "            print(f\"Error in extracted xml for {file}: {e}\")\n",
    "            break\n",
    "        fp.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODOs:\n",
    "- merge text, spans, and lists that overflow onto the next page or next TEXT element\n",
    "    - handle sublist -- [Slack](https://nava.slack.com/archives/C06DP498D1D/p1724182273941319?thread_ts=1723826732.335659&cid=C06DP498D1D)\n",
    "- parsing tables (large/med effort)\n",
    "- remove stop words from `tags`\n",
    "extract-hyperlinks-from-pdf-in-python\n",
    "- test pdfminer's image extraction (e.g., 105.pdf page 7) \n",
    "- extract weblinks for hyperlinked text: https://stackoverflow.com/questions/27744210/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring hyperlink identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pdf=f\"{pdf_folder}/100.pdf\"\n",
    "fp = open(my_pdf, \"rb\")\n",
    "doc = PDFDocument(PDFParser(fp))\n",
    "doc.catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.pdftypes import dict_value\n",
    "\n",
    "entry=doc.catalog['Outlines']\n",
    "entry_d = dict_value(entry)\n",
    "print(entry_d)\n",
    "entry_d2 = dict_value(entry_d['Last'])\n",
    "print(entry_d2)\n",
    "dest = dict_value(entry_d2['Dest'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entry=doc.catalog['Pages']\n",
    "entry_d = dict_value(entry)\n",
    "print(entry_d)\n",
    "print(len(entry_d['Kids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, k in enumerate(entry_d['Kids']):\n",
    "    print(i, pprint.pformat(dict_value(k)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_ref=entry_d['Kids'][9]\n",
    "page=dict_value(page_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_value(page['Annots'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.pdftypes import PDFObjRef\n",
    "\n",
    "def get_page_links(page: PDFObjRef):\n",
    "    annotationList = []\n",
    "    if 'Annots' in page:\n",
    "        for annotation_ref in page['Annots']:\n",
    "            annotationDict = dict_value(annotation_ref)\n",
    "            # print(annotationDict)\n",
    "            # print(annotationDict[\"Subtype\"])\n",
    "            if str(annotationDict[\"Subtype\"]) != \"/'Link'\":\n",
    "                # Skip over any annotations that are not links\n",
    "                continue\n",
    "            position = annotationDict[\"Rect\"]\n",
    "            uriDict = dict_value(annotationDict[\"A\"])\n",
    "            # This has always been true so far.\n",
    "            # print(uriDict)\n",
    "            assert str(uriDict[\"S\"]) == \"/'URI'\"\n",
    "            # Some of my URI's have spaces.\n",
    "            uri = str(uriDict[\"URI\"]).replace(\" \", \"%20\")\n",
    "            # print(uri)\n",
    "            annotationList.append((position, uri))\n",
    "    return annotationList\n",
    "\n",
    "entry=doc.catalog['Pages']\n",
    "for p, k in  enumerate(dict_value(entry)['Kids'], start=1):\n",
    "    page = dict_value(k)\n",
    "    print(p, get_page_links(page))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
