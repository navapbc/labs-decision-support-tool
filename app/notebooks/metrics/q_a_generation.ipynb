{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Documents from database and generate questions based on chunk content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74b3d3a1394641b4826b637d4c208e9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=5, description='Number of questions per chunk/document:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e08407ce5ba44fac8e6c20ad7e07e11b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='OpenAI LLM Model:', index=1, options=('gpt-3.5-turbo-instruct', 'gpt-4o', 'gpt-4o-mini')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ff83f5dd6764a63832d6419798444ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(description='Question source:', index=1, options=('document', 'chunk'), value='chunk')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edcd81f3645e4133be741808ec40d6f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SelectMultiple(description='Dataset source:', index=(0, 1), options=('Imagine LA', 'CA EDD', 'BEM'), value=('I…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f027a2fe184047bcb7dcba677df70b05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='question_answer_pairs.csv', description='Filename:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import logging\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from notebooks.metrics.generate_q_a_pairs import process_document_or_chunk, write_question_answer_json_to_csv\n",
    "from src.db.models.document import Document\n",
    "from src.app_config import app_config\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "num_qa_per_chunk_or_doc = widgets.IntText(\n",
    "    value=5,\n",
    "    description='Number of questions per chunk/document:',\n",
    "    disabled=False   \n",
    ")\n",
    "\n",
    "llm_model = widgets.Dropdown(\n",
    "    options=['gpt-3.5-turbo-instruct', 'gpt-4o', 'gpt-4o-mini'],\n",
    "    value='gpt-4o',\n",
    "    description='OpenAI LLM Model:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "question_gen_selection = widgets.RadioButtons(\n",
    "    options=['document', 'chunk'],\n",
    "    value='chunk',\n",
    "    description='Question source:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "question_dataset = widgets.SelectMultiple(\n",
    "    options=['Imagine LA', 'CA EDD', 'BEM'],\n",
    "    value=['Imagine LA', 'CA EDD' ],\n",
    "    description='Dataset source:',\n",
    "    disabled=False\n",
    ")\n",
    "file_name = widgets.Text(\n",
    "    value='question_answer_pairs.csv',\n",
    "    description='Filename:',\n",
    "    disabled=False   \n",
    ")\n",
    "\n",
    "display(\n",
    "    num_qa_per_chunk_or_doc,\n",
    "    llm_model,\n",
    "    question_gen_selection,\n",
    "    question_dataset,\n",
    "    file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-05 14:31:43,003 - INFO - Constructed database configuration\n",
      "2024-12-05 14:31:43,051 - INFO - connected to postgres db\n",
      "2024-12-05 14:31:43,052 - WARNING - database connection is not using SSL\n",
      "2024-12-05 14:31:43,063 - INFO - Start processing with llm gpt-4o-mini\n",
      "\u001b[92m14:31:43 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:31:43,076 - INFO - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:31:45,353 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:31:45 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-12-05 14:31:45,366 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:31:45 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:31:45,379 - INFO - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:31:47,342 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:31:47 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-12-05 14:31:47,349 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:31:47 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:31:47,363 - INFO - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:31:49,652 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:31:49 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-12-05 14:31:49,657 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:31:49 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:31:49,667 - INFO - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:31:51,185 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:31:51 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-12-05 14:31:51,193 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:31:51 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:31:51,203 - INFO - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:31:53,134 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:31:53 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-12-05 14:31:53,139 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:31:53 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:31:53,147 - INFO - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:31:54,873 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:31:54 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-12-05 14:31:54,878 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:31:54 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:31:54,892 - INFO - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:31:56,508 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:31:56 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-12-05 14:31:56,514 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:31:56 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:31:56,527 - INFO - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:31:58,771 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:31:58 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-12-05 14:31:58,777 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:31:58 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:31:58,786 - INFO - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:32:00,915 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:32:00 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-12-05 14:32:00,921 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:32:00 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:32:00,928 - INFO - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:32:03,371 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:32:03 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-12-05 14:32:03,377 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:32:03 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:32:03,387 - INFO - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:32:05,729 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:32:05 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-12-05 14:32:05,736 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:32:05 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:32:05,749 - INFO - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:32:07,673 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:32:07 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-12-05 14:32:07,680 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:32:07 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:32:07,689 - INFO - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:32:10,946 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:32:10 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-12-05 14:32:10,952 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:32:10 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:32:10,966 - INFO - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:32:12,575 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:32:12 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-12-05 14:32:12,581 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:32:12 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:32:12,590 - INFO - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:32:14,847 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:32:14 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-12-05 14:32:14,857 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:32:14 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:32:14,881 - INFO - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:32:17,194 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:32:17 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-12-05 14:32:17,203 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:32:17 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:32:17,212 - INFO - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:32:18,522 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:32:18 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-12-05 14:32:18,525 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:32:18 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:32:18,532 - INFO - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:32:19,959 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:32:19 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-12-05 14:32:19,966 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:32:19 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:32:19,983 - INFO - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:32:23,029 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:32:23 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-12-05 14:32:23,035 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:32:23 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:32:23,046 - INFO - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:32:25,898 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:32:25 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-12-05 14:32:25,904 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:32:25 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:32:25,919 - INFO - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:32:28,256 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:32:28 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-12-05 14:32:28,264 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:32:28 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:32:28,277 - INFO - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:32:30,300 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:32:30 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-12-05 14:32:30,307 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:32:30 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:32:30,319 - INFO - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:32:31,835 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:32:31 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-12-05 14:32:31,842 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:32:31 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:32:31,854 - INFO - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:32:37,572 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:32:37 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-12-05 14:32:37,579 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:32:37 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:32:37,591 - INFO - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:32:38,902 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:32:38 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-12-05 14:32:38,911 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:32:38 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:32:38,927 - INFO - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:32:40,861 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:32:40 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-12-05 14:32:40,867 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:32:40 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:32:40,881 - INFO - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:32:42,947 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:32:42 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-12-05 14:32:42,952 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:32:42 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:32:42,966 - INFO - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:32:45,562 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:32:45 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-12-05 14:32:45,569 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:32:45 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:32:45,584 - INFO - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:32:49,225 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:32:49 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-12-05 14:32:49,236 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:32:49 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:32:49,249 - INFO - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:32:51,601 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:32:51 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-12-05 14:32:51,607 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:32:51 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:32:51,620 - INFO - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:32:53,854 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:32:53 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-12-05 14:32:53,861 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:32:53 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:32:53,871 - INFO - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:32:56,005 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:32:56 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-12-05 14:32:56,010 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:32:56 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:32:56,022 - INFO - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:32:58,671 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:32:58 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-12-05 14:32:58,687 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:32:58 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:32:58,710 - INFO - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:33:01,021 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:33:01 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-12-05 14:33:01,027 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:33:01 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:33:01,035 - INFO - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:33:02,352 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:33:02 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-12-05 14:33:02,359 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:33:02 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:33:02,372 - INFO - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:33:04,810 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:33:04 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-12-05 14:33:04,816 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:33:04 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:33:04,825 - INFO - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:33:06,967 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:33:06 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-12-05 14:33:06,968 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:33:06 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:33:06,974 - INFO - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:33:08,906 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:33:08 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-12-05 14:33:08,913 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:33:08 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:33:08,926 - INFO - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:33:10,343 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:33:10 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-12-05 14:33:10,349 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:33:10 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:33:10,361 - INFO - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:33:11,775 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:33:11 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-12-05 14:33:11,779 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:33:11 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:33:11,789 - INFO - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:33:13,514 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:33:13 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-12-05 14:33:13,519 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:33:13 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:33:13,529 - INFO - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:33:15,564 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:33:15 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-12-05 14:33:15,571 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:33:15 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:33:15,583 - INFO - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:33:17,511 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:33:17 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-12-05 14:33:17,518 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:33:17 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:33:17,530 - INFO - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:33:19,661 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:33:19 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-12-05 14:33:19,664 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:33:19 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:33:19,677 - INFO - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:33:21,709 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:33:21 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-12-05 14:33:21,712 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:33:21 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:33:21,720 - INFO - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:33:23,475 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:33:23 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-12-05 14:33:23,483 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:33:23 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:33:23,497 - INFO - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:33:25,267 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:33:25 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-12-05 14:33:25,275 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:33:25 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:33:25,286 - INFO - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:33:26,928 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:33:26 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-12-05 14:33:26,929 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:33:26 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:33:26,932 - INFO - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:33:28,980 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:33:28 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-12-05 14:33:28,985 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:33:29 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:33:29,000 - INFO - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:33:30,514 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:33:30 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-12-05 14:33:30,520 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:33:30 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:33:30,530 - INFO - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:33:32,563 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:33:32 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-12-05 14:33:32,569 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:33:32 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:33:32,583 - INFO - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:33:34,363 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:33:34 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-12-05 14:33:34,370 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:33:34 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:33:34,380 - INFO - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:33:36,294 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:33:36 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-12-05 14:33:36,301 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:33:36 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:33:36,310 - INFO - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:33:38,111 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:33:38 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-12-05 14:33:38,120 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:33:38 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:33:38,143 - INFO - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:33:40,651 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:33:40 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-12-05 14:33:40,658 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:33:40 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:33:40,667 - INFO - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:33:43,009 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:33:43 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-12-05 14:33:43,015 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:33:43 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:33:43,027 - INFO - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:33:44,862 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:33:44 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-12-05 14:33:44,868 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:33:44 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:33:44,883 - INFO - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:33:47,101 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:33:47 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-12-05 14:33:47,105 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:33:47 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:33:47,113 - INFO - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:33:48,947 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:33:48 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-12-05 14:33:48,954 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:33:48 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:33:48,966 - INFO - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2024-12-05 14:33:50,994 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:33:51 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-12-05 14:33:51,000 - INFO - Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing, output saved to question_answer_pairs.csv\n"
     ]
    }
   ],
   "source": [
    "with app_config.db_session() as db_session:\n",
    "    selected_dataset = list(question_dataset.value)\n",
    "    documents = db_session.query(Document).filter(Document.dataset.in_(selected_dataset)).all()\n",
    "\n",
    "    fields = [\"question\", \"answer\", \"document_name\", \"document_source\", \"dataset\", \"document_id\", \"chunk_id\", \"content_hash\"]\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "    logging.info(f'Start processing with llm {llm_model.value}')\n",
    "\n",
    "    dataset_list = \", \".join(question_dataset.value)\n",
    "    for document in documents:\n",
    "        chunk_list= document.chunks\n",
    "        if question_gen_selection.value == \"chunk\":\n",
    "            for chunk in chunk_list:\n",
    "                chunk_q_a_json = process_document_or_chunk(document_or_chunk=chunk, num_of_chunks=num_qa_per_chunk_or_doc.value, llm=llm_model.value, dataset=dataset_list)\n",
    "                write_question_answer_json_to_csv(file_name.value, fields, chunk_q_a_json)        \n",
    "        else:\n",
    "            document_q_a_json = process_document_or_chunk(document_or_chunk=document, num_of_chunks=num_qa_per_chunk_or_doc.value,llm=llm_model.value, dataset=dataset_list)\n",
    "            write_question_answer_json_to_csv(file_name.value, fields, document_q_a_json)        \n",
    "    print(f\"Finished processing, output saved to {file_name.value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
