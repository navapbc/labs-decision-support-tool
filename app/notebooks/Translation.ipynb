{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1369e75c-262b-4b33-9ecc-49b50dc3fdae",
   "metadata": {},
   "source": [
    "# Generate Q+A pairs\n",
    "\n",
    "For each chunk in the database, use an LLM (GPT-4o-mini) to generate a question that would plausibly be answered using this chunk. To speed this up, make the API calls in parallel.\n",
    "\n",
    "Stores these in `generated_questions.json`, since this hits the OpenAI API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7eae72ce-d4eb-46a3-a2c5-cac60d1df1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "database connection is not using SSL\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import select\n",
    "import asyncio\n",
    "import json\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "from src.app_config import app_config\n",
    "from src.db.models.document import Chunk\n",
    "\n",
    "client = AsyncOpenAI()\n",
    "\n",
    "MODEL=\"gpt-4o-mini\"\n",
    "PROMPT = \"\"\"Generate a question that can be derived from information in this content:\n",
    "\n",
    "<content>\n",
    "{content}\n",
    "</content>\n",
    "\n",
    "Do not generate a question that cannot be answered by the content.\"\"\"\n",
    "\n",
    "# Crop chunk.content to this length -- 128 is the max_seq_length of distiluse-base-multilingual-cased-v2\n",
    "# and 5 is about the average word length in our corpus\n",
    "max_content_length = 128 * 5\n",
    "\n",
    "# Or, essentially no limit\n",
    "#max_content_length = 1_000_000\n",
    "\n",
    "async def generate_qa(\n",
    "    chunk: Chunk,\n",
    "    semaphore: asyncio.Semaphore,\n",
    ") -> str:\n",
    "    async with semaphore:\n",
    "        completion =  await client.chat.completions.create(\n",
    "                model=MODEL,\n",
    "                messages=[{\"role\": \"user\", \"content\": PROMPT.format(content=chunk.content[:max_content_length])}]\n",
    "            )\n",
    "        return completion.choices[0].message.content\n",
    "\n",
    "async def create_synthetic_questions(\n",
    "    chunks: list[Chunk],\n",
    "    max_concurrency: int = 10,\n",
    ") -> list[str]:\n",
    "    semaphore = asyncio.Semaphore(max_concurrency)\n",
    "    tasks = [\n",
    "        generate_qa(chunk, semaphore)\n",
    "        for chunk in chunks\n",
    "    ]\n",
    "    return await asyncio.gather(*tasks, return_exceptions=True)\n",
    "\n",
    "with app_config.db_session() as db_session:\n",
    "    chunks = db_session.execute(select(Chunk).limit(500)).scalars().all()\n",
    "    questions = await create_synthetic_questions(chunks)\n",
    "    content_and_questions = list(zip([c.content[:max_content_length] for c in chunks], questions))\n",
    "    \n",
    "    with open(f\"translation/generated_questions_{max_content_length}.json\", \"w\") as file:\n",
    "        json.dump(content_and_questions, file, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb56aa9-a08d-40bf-9ebb-4ecca5e89843",
   "metadata": {},
   "source": [
    "# Evaluate retrieval from stored texts and questions\n",
    "\n",
    "Load the chunk texts and questions from the file, create embeddings for each, and calculate recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bca94a76-af8c-410e-9a6b-720b44baadcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import semantic_search\n",
    "\n",
    "def generate_embeddings(model_name, chunk_contents, questions):\n",
    "    model = SentenceTransformer(model_name)\n",
    "    return model.encode(chunk_contents), model.encode(questions)\n",
    "\n",
    "def compute_recall(question_embeddings, content_embeddings, top_k=5):\n",
    "\n",
    "    # Search Top K for each question\n",
    "    search_results = semantic_search(question_embeddings, content_embeddings, top_k=top_k)\n",
    "    \n",
    "    # Was the chunk_index found in the results?\n",
    "    # Note that chunk_index is the same as question_index because chunk_contents[i] is chunk corresponding with questions[i]\n",
    "    # search_result is a list of the top k most similar entries in chunk_contents for each question in questions\n",
    "    # in other words, it has dimensions [len(questions)][k]\n",
    "    found_content = [\n",
    "        chunk_index in [result['corpus_id'] for result in search_result]\n",
    "    for chunk_index, search_result in enumerate(search_results)]\n",
    "\n",
    "    return sum(found_content)/len(found_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "570ce337-9910-458e-97f2-bf1185296c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPNet recall:  0.864\n",
      "Distiluse recall:  0.826\n"
     ]
    }
   ],
   "source": [
    "# Slow!\n",
    "\n",
    "with open(\"translation/generated_questions_640.json\", \"r\") as file:\n",
    "    content_and_questions = json.load(file)\n",
    "\n",
    "chunk_contents = [cq_pair[0] for cq_pair in content_and_questions]\n",
    "questions = [cq_pair[1] for cq_pair in content_and_questions]\n",
    "\n",
    "content_embeddings, question_embeddings = generate_embeddings(\"multi-qa-mpnet-base-cos-v1\", chunk_contents, questions)\n",
    "r = compute_recall(content_embeddings, question_embeddings)\n",
    "print(\"MPNet recall: \", r)\n",
    "\n",
    "content_embeddings, question_embeddings = generate_embeddings(\"distiluse-base-multilingual-cased-v2\", chunk_contents, questions)\n",
    "r = compute_recall(content_embeddings, question_embeddings)\n",
    "print(\"Distiluse recall: \", r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "642538e6-372c-4735-8a6d-8b76d1cb18b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPNet recall:  0.772\n",
      "Distiluse recall:  0.706\n"
     ]
    }
   ],
   "source": [
    "with open(\"translation/generated_questions_1000000.json\", \"r\") as file:\n",
    "    content_and_questions = json.load(file)\n",
    "\n",
    "chunk_contents = [cq_pair[0] for cq_pair in content_and_questions]\n",
    "questions = [cq_pair[1] for cq_pair in content_and_questions]\n",
    "\n",
    "content_embeddings, question_embeddings = generate_embeddings(\"multi-qa-mpnet-base-cos-v1\", chunk_contents, questions)\n",
    "r = compute_recall(content_embeddings, question_embeddings)\n",
    "print(\"MPNet recall: \", r)\n",
    "\n",
    "content_embeddings, question_embeddings = generate_embeddings(\"distiluse-base-multilingual-cased-v2\", chunk_contents, questions)\n",
    "r = compute_recall(content_embeddings, question_embeddings)\n",
    "print(\"Distiluse recall: \", r)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
