{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract Documents from database and generate questions based on chunk number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "database connection is not using SSL\n",
      "2024-11-21 13:56:00,589 - INFO - Start processing\n",
      "\u001b[92m13:56:00 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 13:56:00,601 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 13:56:06,601 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:56:06 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 13:56:06,623 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:56:06 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 13:56:06,634 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 13:56:14,483 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:56:14 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 13:56:14,488 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:56:14 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 13:56:14,498 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 13:56:17,862 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:56:17 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 13:56:17,868 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:56:17 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 13:56:17,883 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 13:56:23,678 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:56:23 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 13:56:23,683 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:56:23 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 13:56:23,693 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 13:56:32,472 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:56:32 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 13:56:32,478 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:56:32 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 13:56:32,487 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 13:56:37,941 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:56:37 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 13:56:37,949 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:56:37 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 13:56:37,961 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 13:56:42,463 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:56:42 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 13:56:42,469 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:56:42 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 13:56:42,478 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 13:56:45,110 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:56:45 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 13:56:45,115 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:56:45 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 13:56:45,129 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 13:56:48,306 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:56:48 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 13:56:48,310 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:56:48 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 13:56:48,321 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 13:56:50,360 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:56:50 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 13:56:50,364 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:56:50 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 13:56:50,378 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 13:56:55,567 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:56:55 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 13:56:55,570 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:56:55 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 13:56:55,576 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 13:56:59,564 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:56:59 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 13:56:59,569 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:56:59 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 13:56:59,576 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 13:57:04,669 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:57:04 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 13:57:04,676 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:57:04 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 13:57:04,696 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 13:57:08,964 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:57:08 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 13:57:08,967 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:57:08 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 13:57:08,973 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 13:57:12,157 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:57:12 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 13:57:12,162 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:57:12 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 13:57:12,177 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 13:57:24,230 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:57:24 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 13:57:24,346 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:57:24 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 13:57:24,356 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 13:57:31,036 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:57:31 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 13:57:31,040 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:57:31 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 13:57:31,048 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 13:57:38,839 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:57:38 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 13:57:38,842 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:57:38 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 13:57:38,847 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 13:57:47,063 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:57:47 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 13:57:47,068 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:57:47 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 13:57:47,076 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 13:58:01,193 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:58:01 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 13:58:01,196 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:58:01 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 13:58:01,211 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 13:58:03,428 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:58:03 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 13:58:03,432 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:58:03 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 13:58:03,445 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 13:58:08,389 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:58:08 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 13:58:08,398 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:58:08 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 13:58:08,407 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 13:58:13,101 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:58:13 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 13:58:13,109 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:58:13 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 13:58:13,122 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 13:58:15,134 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:58:15 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 13:58:15,137 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:58:15 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 13:58:15,150 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 13:58:23,451 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:58:23 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 13:58:23,456 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:58:23 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 13:58:23,463 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 13:58:30,714 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:58:30 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 13:58:30,720 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:58:30 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 13:58:30,729 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 13:58:40,357 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:58:40 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 13:58:40,361 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:58:40 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 13:58:40,380 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 13:58:42,032 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:58:42 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 13:58:42,037 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:58:42 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 13:58:42,052 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 13:58:47,510 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:58:47 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 13:58:47,515 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:58:47 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 13:58:47,523 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 13:59:05,706 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:59:05 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 13:59:05,711 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:59:05 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 13:59:05,718 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 13:59:14,924 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:59:14 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 13:59:14,928 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:59:14 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 13:59:14,935 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 13:59:26,998 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:59:27 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 13:59:27,005 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:59:27 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 13:59:27,016 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 13:59:35,010 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:59:35 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 13:59:35,014 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:59:35 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 13:59:35,021 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 13:59:38,231 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:59:38 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 13:59:38,236 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:59:38 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 13:59:38,248 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 13:59:40,594 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:59:40 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 13:59:40,599 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:59:40 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 13:59:40,614 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 13:59:48,374 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:59:48 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 13:59:48,378 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:59:48 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 13:59:48,387 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 14:00:08,177 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:00:08 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 14:00:08,182 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:00:08 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 14:00:08,191 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 14:00:22,677 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:00:22 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 14:00:22,688 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:00:22 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 14:00:22,699 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 14:00:40,247 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:00:40 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 14:00:40,252 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:00:40 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 14:00:40,261 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 14:00:58,233 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:00:58 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 14:00:58,241 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:00:58 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 14:00:58,250 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 14:01:16,067 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:01:16 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 14:01:16,073 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:01:16 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 14:01:16,081 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 14:01:28,863 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:01:28 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 14:01:28,868 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:01:28 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 14:01:28,905 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 14:01:37,643 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:01:37 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 14:01:37,648 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:01:37 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 14:01:37,656 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 14:01:45,965 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:01:45 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 14:01:45,968 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:01:45 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 14:01:45,974 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from sqlalchemy import select\n",
    "\n",
    "from notebooks.question_answer_generator.generate_q_a_pairs import generate_question_answer_pair, write_question_answer_json_to_csv\n",
    "from src.db.models.document import Document\n",
    "from src.app_config import app_config\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "question_gen_selection = input(\"Generate questions by chunk or document?\")\n",
    "\n",
    "with app_config.db_session() as db_session:\n",
    "    documents = db_session.execute(select(Document)).scalars().all()\n",
    "    fields = [\"question\", \"answer\", \"document_name\", \"document_source\", \"document_id\"]\n",
    "    logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "    logging.info('Start processing')\n",
    "\n",
    "    for document in documents:\n",
    "        chunk_list= document.chunks\n",
    "        if question_gen_selection == \"chunk\":\n",
    "            for chunk in chunk_list:\n",
    "                chunk_q_a_json = generate_question_answer_pair(document=chunk, num_of_chunks=chunk.num_splits)\n",
    "                write_question_answer_json_to_csv(\"question_answer_pairs.csv\", fields, chunk_q_a_json)        \n",
    "        else:\n",
    "            document_q_a_json = generate_question_answer_pair(document=document, num_of_chunks=len(document.chunks))\n",
    "            write_question_answer_json_to_csv(\"question_answer_pairs.csv\", fields, document_q_a_json)        \n",
    "    logger.info(\"Finished processing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
