{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract Documents from database and generate questions based on chunk number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate questions by chunk or document? chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "database connection is not using SSL\n",
      "2024-11-21 20:22:17,424 - INFO - Start processing\n",
      "\u001b[92m20:22:17 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:22:17,435 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:22:20,269 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:22:20 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:22:20,288 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:22:20 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:22:20,298 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:22:23,700 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:22:23 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:22:23,707 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:22:23 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:22:23,716 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:22:27,028 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:22:27 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:22:27,033 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:22:27 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:22:27,049 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:22:34,527 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:22:34 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:22:34,531 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:22:34 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:22:34,538 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:22:44,687 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:22:44 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:22:44,692 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:22:44 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:22:44,699 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:22:50,270 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:22:50 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:22:50,275 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:22:50 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:22:50,291 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:22:55,425 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:22:55 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:22:55,430 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:22:55 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:22:55,438 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:22:58,537 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:22:58 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:22:58,544 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:22:58 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:22:58,557 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:23:05,435 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:23:05 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:23:05,440 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:23:05 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:23:05,455 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:23:07,372 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:23:07 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:23:07,378 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:23:07 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:23:07,391 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:23:15,023 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:23:15 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:23:15,028 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:23:15 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:23:15,035 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:23:20,581 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:23:20 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:23:20,586 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:23:20 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:23:20,595 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:23:27,023 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:23:27 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:23:27,028 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:23:27 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:23:27,042 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:23:31,818 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:23:31 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:23:31,823 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:23:31 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:23:31,830 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:23:36,046 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:23:36 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:23:36,051 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:23:36 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:23:36,065 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:23:55,644 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:23:55 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:23:55,648 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:23:55 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:23:55,656 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:24:05,166 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:24:05 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:24:05,172 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:24:05 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:24:05,180 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:24:15,571 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:24:15 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:24:15,574 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:24:15 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:24:15,585 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:24:25,710 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:24:25 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:24:25,715 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:24:25 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:24:25,723 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:24:34,777 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:24:34 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:24:34,783 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:24:34 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:24:34,797 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:24:36,686 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:24:36 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:24:36,689 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:24:36 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:24:36,699 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:24:45,820 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:24:45 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:24:45,823 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:24:45 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:24:45,830 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:24:55,720 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:24:55 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:24:55,723 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:24:55 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:24:55,732 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:24:58,029 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:24:58 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:24:58,033 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:24:58 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:24:58,054 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:25:04,930 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:25:04 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:25:04,936 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:25:04 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:25:04,942 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:25:13,629 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:25:13 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:25:13,633 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:25:13 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:25:13,640 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:25:22,221 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:25:22 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:25:22,226 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:25:22 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:25:22,240 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:25:23,829 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:25:23 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:25:23,835 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:25:23 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:25:23,848 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:25:28,480 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:25:28 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:25:28,483 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:25:28 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:25:28,488 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:25:39,848 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:25:39 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:25:39,853 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:25:39 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:25:39,861 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:25:52,401 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:25:52 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:25:52,406 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:25:52 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:25:52,413 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:26:07,393 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:26:07 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:26:07,397 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:26:07 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:26:07,410 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:26:13,740 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:26:13 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:26:13,742 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:26:13 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:26:13,747 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:26:17,736 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:26:17 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:26:17,740 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:26:17 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:26:17,753 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:26:20,320 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:26:20 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:26:20,325 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:26:20 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:26:20,340 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:26:33,307 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:26:33 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:26:33,312 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:26:33 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:26:33,321 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:26:53,627 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:26:53 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:26:53,631 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:26:53 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:26:53,638 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:27:15,798 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:27:15 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:27:15,802 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:27:15 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:27:15,809 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:27:31,922 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:27:31 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:27:31,927 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:27:31 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:27:31,934 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:27:52,622 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:27:52 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:27:52,626 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:27:52 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:27:52,635 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:28:16,939 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:28:16 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:28:16,953 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:28:16 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:28:16,962 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:28:33,237 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:28:33 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:28:33,242 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:28:33 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:28:33,255 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:28:41,237 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:28:41 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:28:41,243 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:28:41 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:28:41,250 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:28:49,083 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:28:49 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:28:49,086 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:28:49 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:28:49,097 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:28:56,559 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:28:56 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:28:56,564 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:28:56 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:28:56,570 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:29:05,162 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:29:05 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:29:05,166 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:29:05 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:29:05,178 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:29:07,114 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:29:07 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:29:07,119 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:29:07 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:29:07,131 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:29:08,586 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:29:08 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:29:08,591 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:29:08 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:29:08,605 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:29:12,858 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:29:12 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:29:12,863 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:29:12 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:29:12,871 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:29:18,923 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:29:18 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:29:18,927 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:29:18 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:29:18,941 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:29:34,747 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:29:34 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:29:34,753 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:29:34 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:29:34,761 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:29:48,714 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:29:48 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:29:48,718 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:29:48 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:29:48,727 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:30:02,904 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:30:02 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:30:02,915 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:30:02 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:30:02,925 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:30:15,077 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:30:15 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:30:15,082 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:30:15 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:30:15,093 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:30:30,390 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:30:30 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:30:30,395 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:30:30 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:30:30,404 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:30:42,081 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:30:42 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:30:42,086 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:30:42 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:30:42,094 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:30:54,706 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:30:54 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:30:54,710 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:30:54 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:30:54,718 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:31:03,926 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:31:03 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:31:03,931 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:31:03 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:31:03,951 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:31:12,905 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:31:12 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:31:12,908 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:31:12 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:31:12,917 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:31:23,918 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:31:23 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:31:23,922 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:31:23 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:31:23,930 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:31:38,780 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:31:38 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:31:38,784 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:31:38 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:31:38,803 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:31:47,313 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:31:47 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:31:47,317 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:31:47 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:31:47,326 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:32:00,827 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:32:00 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:32:00,831 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:32:00 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:32:00,839 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:32:09,590 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:32:09 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:32:09,594 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:32:09 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:32:09,605 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:32:12,273 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:32:12 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:32:12,277 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:32:12 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:32:12,292 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:32:14,447 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:32:14 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:32:14,452 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:32:14 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:32:14,465 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:32:22,167 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:32:22 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:32:22,170 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:32:22 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:32:22,178 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:32:28,152 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:32:28 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:32:28,157 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:32:28 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:32:28,165 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:32:34,521 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:32:34 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:32:34,524 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:32:34 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:32:34,533 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:32:40,443 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:32:40 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:32:40,447 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:32:40 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:32:40,461 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:32:42,920 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:32:42 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:32:42,925 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:32:42 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:32:42,940 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:32:51,567 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:32:51 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:32:51,571 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:32:51 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:32:51,579 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:32:58,223 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:32:58 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:32:58,228 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:32:58 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:32:58,236 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:33:06,364 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:33:06 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:33:06,368 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:33:06 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:33:06,377 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:33:08,804 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:33:08 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:33:08,809 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:33:08 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:33:08,822 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:33:10,564 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:33:10 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:33:10,569 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:33:10 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:33:10,581 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:33:13,843 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:33:13 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:33:13,847 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:33:13 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:33:13,860 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:33:18,246 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:33:18 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:33:18,251 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:33:18 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:33:18,259 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:33:20,622 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:33:20 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:33:20,626 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:33:20 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:33:20,637 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:33:23,379 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:33:23 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:33:23,383 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:33:23 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:33:23,398 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:33:32,793 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:33:32 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:33:32,797 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:33:32 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:33:32,805 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:33:40,547 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:33:40 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:33:40,551 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:33:40 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:33:40,560 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:33:45,258 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:33:45 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:33:45,263 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:33:45 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:33:45,277 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:33:51,259 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:33:51 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:33:51,264 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:33:51 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:33:51,270 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:33:54,450 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:33:54 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:33:54,455 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:33:54 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:33:54,475 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:34:31,976 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:34:31 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:34:31,982 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:34:31 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:34:31,990 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:34:38,709 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:34:38 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:34:38,713 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:34:38 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:34:38,721 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:34:47,231 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:34:47 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:34:47,237 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:34:47 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:34:47,243 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:35:05,460 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:35:05 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:35:05,466 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:35:05 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:35:05,472 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:35:13,972 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:35:13 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:35:13,975 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:35:13 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:35:13,981 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:35:22,353 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:35:22 - LiteLLM:INFO\u001b[0m: utils.py:889 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-21 20:35:22,357 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:35:22 - LiteLLM:INFO\u001b[0m: utils.py:2720 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2024-11-21 20:35:22,363 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from sqlalchemy import select\n",
    "\n",
    "from notebooks.question_answer_generator.generate_q_a_pairs import process_document_or_chunk, write_question_answer_json_to_csv\n",
    "from src.db.models.document import Document\n",
    "from src.app_config import app_config\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "question_gen_selection = input(\"Generate questions by chunk or document?\")\n",
    "\n",
    "with app_config.db_session() as db_session:\n",
    "    documents = db_session.execute(select(Document)).scalars().all()\n",
    "    fields = [\"question\", \"answer\", \"document_name\", \"document_source\", \"document_id\"]\n",
    "    logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "    logging.info('Start processing')\n",
    "\n",
    "    for document in documents:\n",
    "        chunk_list= document.chunks\n",
    "        if question_gen_selection == \"chunk\":\n",
    "            for chunk in chunk_list:\n",
    "                chunk_q_a_json = process_document_or_chunk(document=chunk, num_of_chunks=chunk.num_splits)\n",
    "                write_question_answer_json_to_csv(\"question_answer_pairs.csv\", fields, chunk_q_a_json)        \n",
    "        else:\n",
    "            document_q_a_json = process_document_or_chunk(document=document, num_of_chunks=len(document.chunks))\n",
    "            write_question_answer_json_to_csv(\"question_answer_pairs.csv\", fields, document_q_a_json)        \n",
    "    logger.info(\"Finished processing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
