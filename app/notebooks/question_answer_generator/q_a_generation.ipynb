{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Documents from database and generate questions based on chunk number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from sqlalchemy import select\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from notebooks.question_answer_generator.generate_q_a_pairs import process_document_or_chunk, write_question_answer_json_to_csv\n",
    "from src.db.models.document import Document\n",
    "from src.app_config import app_config\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "num_qa_per_chunk_or_doc = widgets.IntText(\n",
    "    value=5,\n",
    "    description='Number of questions per chunk/document:',\n",
    "    disabled=False   \n",
    ")\n",
    "display(num_qa_per_chunk_or_doc)\n",
    "\n",
    "llm_model = widgets.Dropdown(\n",
    "    options=['gpt-3.5-turbo-instruct', 'gpt-4o', 'gpt-4o-mini'],\n",
    "    value='gpt-4o',\n",
    "    description='OpenAI LLM Model:',\n",
    "    disabled=False,\n",
    ")\n",
    "display(llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_gen_selection = input(\"Generate questions by chunk or document?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with app_config.db_session() as db_session:\n",
    "    documents = db_session.execute(select(Document)).scalars().all()\n",
    "    fields = [\"question\", \"answer\", \"document_name\", \"document_source\", \"document_id\", \"chunk_id\"]\n",
    "    logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "    logging.info(f'Start processing with llm {llm_model.value}')\n",
    "\n",
    "    for document in documents:\n",
    "        chunk_list= document.chunks\n",
    "        if question_gen_selection == \"chunk\":\n",
    "            for chunk in chunk_list:\n",
    "                chunk_q_a_json = process_document_or_chunk(document=chunk, num_of_chunks=num_qa_per_chunk_or_doc.value, llm=llm_model.value)\n",
    "                write_question_answer_json_to_csv(\"question_answer_pairs.csv\", fields, chunk_q_a_json)        \n",
    "        else:\n",
    "            document_q_a_json = process_document_or_chunk(document=document, num_of_chunks=num_qa_per_chunk_or_doc.value,llm=llm_model.value)\n",
    "            write_question_answer_json_to_csv(\"question_answer_pairs.csv\", fields, document_q_a_json)        \n",
    "    logger.info(\"Finished processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
